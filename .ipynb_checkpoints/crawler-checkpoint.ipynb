{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import sys\n",
    "import string\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets scraped: 60\n",
      "Last Tweet: {'Doc_id': 268848975, 'Message': '$AAPL NEW ARTICLE : The Stock Market Was Amazing in 2020 -- Let Us Count the Ways https://www.stck.pro/news/AAPL/10019216', 'Date': '2020-12-31', 'Time': '22:39:05Z', 'Symbols': ['AAPL'], 'Username': 'STCKPRO', 'Name': 'STCK.PRO', 'Sentiment': 'N/A'}\n"
     ]
    }
   ],
   "source": [
    "def crawler(ticker, latest_xhr_id='268864272', max_volume=50000):   \n",
    "    inv_table = {}\n",
    "    \n",
    "    # Push Errors if scrape volume less than 0:\n",
    "    if max_volume <= 0:\n",
    "        sys.exit(\"Error: max_volume must be more than 0\")\n",
    "\n",
    "    master_content = []  # List to store all data extracted\n",
    "    scroll_list = [latest_xhr_id]  # List to store all XHR id to be part of the url parameters\n",
    "    tracker = 0\n",
    "    fail_count = 0\n",
    "\n",
    "    # Running for loop for collecting data from stocktwits. Each loop collects 20 comments.\n",
    "    for _ in range(max_volume):\n",
    "        try:\n",
    "            headers = {\n",
    "                'authority': 'api.stocktwits.com',\n",
    "                'accept': 'application/json',\n",
    "                'authorization': 'OAuth 6439333424451d1c85e731fb126006f7780192d2',\n",
    "                'user-agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                               'Chrome/87.0.4280.88 Safari/537.36'),\n",
    "                'origin': 'https://stocktwits.com',\n",
    "                'sec-fetch-site': 'same-site',\n",
    "                'sec-fetch-mode': 'cors',\n",
    "                'sec-fetch-dest': 'empty',\n",
    "                'referer': 'https://stocktwits.com/',\n",
    "                'accept-language': 'en-US,en;q=0.9',\n",
    "            }`\n",
    "\n",
    "            params = (\n",
    "                ('symbols',ticker),\n",
    "                ('filter', 'all'),\n",
    "                ('limit', '30'),\n",
    "                ('max', scroll_list[-1]),\n",
    "                ('since', 224610272)\n",
    "            )\n",
    "\n",
    "            response = requests.get(f'https://api.stocktwits.com/api/2/streams/symbols.json',\n",
    "                                    headers=headers, params=params)\n",
    "            content = response.json()\n",
    "            messages = content['messages']\n",
    "            # Creating dictionary for items scraped\n",
    "            for item in messages:\n",
    "                content_dict = {}\n",
    "                content_dict['Doc_id'] = item['id']\n",
    "                content_dict['Message'] = item['body']\n",
    "                content_dict['Date'] = item['created_at'].split('T')[0]\n",
    "                content_dict['Time'] = item['created_at'].split('T')[1]\n",
    "                content_dict['Symbols'] = []\n",
    "                content_dict['Username'] = item['user']['username']\n",
    "                content_dict['Name'] = item['user']['name']\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    content_dict['Sentiment'] = item['entities']['sentiment']['basic']\n",
    "                except TypeError:\n",
    "                    content_dict['Sentiment'] = \"N/A\"\n",
    "                    \n",
    "                for i in range(len(item['symbols'])):\n",
    "                    label = item['symbols'][i]['symbol']\n",
    "                    content_dict['Symbols'].append(label)\n",
    "                    \n",
    "                    if label in inv_table:\n",
    "                        inv_table[label].append(content_dict['Doc_id'])\n",
    "                    else:\n",
    "                        inv_table[label] = [content_dict['Doc_id']]\n",
    "\n",
    "                master_content.append(content_dict.copy())\n",
    "                    \n",
    "\n",
    "            next_20_id = str(messages[-1]['id'])\n",
    "            scroll_list.append(next_20_id)\n",
    "\n",
    "            # Progress Tracker\n",
    "            tracker += 1\n",
    "        except:\n",
    "            fail_count += 1\n",
    "\n",
    "    print(f\"Number of tweets scraped: {len(master_content)}\")\n",
    "    print(f\"Last Tweet: {master_content[-1]}\")\n",
    "\n",
    "    df = pd.DataFrame(master_content)\n",
    "    \n",
    "    for i in inv_table:\n",
    "        inv_table[i].sort()\n",
    "        \n",
    "    return df,inv_table\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tweets_df,inv_table = scraper (\"AAPL,BABA\",max_volume = 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to update the database \n",
    "tweets_df.to_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset Preparation (Train, Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_1 = pd.read_csv('Database_1mil.csv')\n",
    "tweets_2 = pd.read_csv('Database_2mil.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_1.append(tweets_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(columns = \"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test = tweets.head(100000)\n",
    "tweets_train = tweets.tail(len(tweets)-100000)\n",
    "\n",
    "# Store 100k tweets for DB\n",
    "# tweets_test.to_csv(\"tweets_100k.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Trainset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = ['Bearish','Bullish']\n",
    "tweets_train = tweets_train[tweets_train['Sentiment'].isin(sentiments)]\n",
    "tweets_train = tweets_train.reset_index().drop(columns=\"index\")\n",
    "\n",
    "tweets_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullish = 0\n",
    "bearish = 0\n",
    "neutral = 0\n",
    "for x in tweets_train['Sentiment']:\n",
    "    if x == 'Bearish':\n",
    "        bearish += 1\n",
    "    elif x == 'Bullish':\n",
    "        bullish += 1\n",
    "    else:\n",
    "        neutral += 1\n",
    "    \n",
    "print('Bullish count: ', bullish)\n",
    "print('Bearish count: ', bearish)\n",
    "print('Neutral count: ', neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "randints = []\n",
    "for i in range(bullish + bearish):\n",
    "    randints.append(random.randint(1, bullish))\n",
    "    \n",
    "tweets_train['randint'] = randints\n",
    "\n",
    "rows_to_drop = []\n",
    "\n",
    "for i, r in tweets_train.iterrows():\n",
    "  if r['Sentiment'] == 'Bullish':\n",
    "    if r['randint'] < (bullish - bearish):\n",
    "      rows_to_drop.append(i)\n",
    "    \n",
    "tweets_train = tweets_train.drop(rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullish = 0\n",
    "bearish = 0\n",
    "neutral = 0\n",
    "for x in tweets_train['Sentiment']:\n",
    "    if x == 'Bearish':\n",
    "        bearish += 1\n",
    "    elif x == 'Bullish':\n",
    "        bullish += 1\n",
    "    else:\n",
    "        neutral += 1\n",
    "    \n",
    "print('Bullish count: ', bullish)\n",
    "print('Bearish count: ', bearish)\n",
    "print('Neutral count: ', neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_date_time(date, time):\n",
    "    time = time[0:len(time)-1]\n",
    "    string = date + ' ' + time\n",
    "    return datetime.datetime.strptime(string, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateIndex = tweets_test.columns.get_loc(\"Date\")\n",
    "timeIndex = tweets_test.columns.get_loc(\"Time\")\n",
    "\n",
    "tweets_test['datetime'] = tweets_test.apply(lambda row: get_date_time(row[dateIndex],row[timeIndex]), axis=1)\n",
    "tweets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_ticker = {\"baba\":\"alibaba\",\n",
    "                  \"amzn\":\"amazon\",\n",
    "                  \"aapl\":\"apple\",\n",
    "                  \"tsla\":\"tesla\",\n",
    "                  \"msft\":\"microsoft\",\n",
    "                  \"fb\":\"facebook\",\n",
    "                  \"googl\":\"google\",\n",
    "                  \"nio\":\"nio\",\n",
    "                  \"twtr\":\"twitter\",\n",
    "                  \"nflx\":\"netflix\"\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_filter_lemmatize(msg):\n",
    "    words = word_tokenize(msg)\n",
    "    \n",
    "    processed = []\n",
    "    \n",
    "    for w in words:\n",
    "        if (w not in stop_words) and (w not in string.punctuation):\n",
    "            if (w in company_ticker):\n",
    "                w = company_ticker[w]\n",
    "            processed.append(lem.lemmatize(w.lower()))\n",
    "            \n",
    "    return processed\n",
    "\n",
    "tweets_train['Words'] = tweets_train['Message'].apply(token_filter_lemmatize)\n",
    "\n",
    "#tweets.to_csv(r'Database.csv', index=False)\n",
    "tweets_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_final = tweets_train[[\"Words\", \"Sentiment\", \"datetime\"]]\n",
    "tweets_train_final.to_csv(\"tweets_train_700k.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Testset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateIndex = tweets_train.columns.get_loc(\"Date\")\n",
    "timeIndex = tweets_train.columns.get_loc(\"Time\")\n",
    "\n",
    "tweets_train['datetime'] = tweets_train.apply(lambda row: get_date_time(row[dateIndex],row[timeIndex]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test['Words'] = tweets_test['Message'].apply(token_filter_lemmatize)\n",
    "tweets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test_final = tweets_test[[\"Words\", \"Sentiment\", \"datetime\"]]\n",
    "tweets_test_final.to_csv(\"tweets_test_100k.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tokenize, Filter and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('tweets_100k.csv')\n",
    "\n",
    "tweets = tweets_df.copy()\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_date_time(date, time):\n",
    "    time = time[0:len(time)-1]\n",
    "    string = date + ' ' + time\n",
    "    return datetime.datetime.strptime(string, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateIndex = tweets.columns.get_loc(\"Date\")\n",
    "timeIndex = tweets.columns.get_loc(\"Time\")\n",
    "\n",
    "tweets['datetime'] = tweets.apply(lambda row: get_date_time(row[dateIndex],row[timeIndex]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest = min(tweets.datetime)\n",
    "total = max(tweets.datetime) - earliest\n",
    "\n",
    "def datetime_score(datetime):\n",
    "    diff = datetime - earliest\n",
    "    return diff/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['datetime_score'] = tweets['datetime'].apply(datetime_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_string(text):\n",
    "    pattern = re.compile('[\\W_]+')\n",
    "    return pattern.sub(' ', text)\n",
    "\n",
    "\n",
    "tweets['Message'] = tweets['Message'].apply(clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_ticker = {\"baba\":\"alibaba\",\n",
    "                  \"amzn\":\"amazon\",\n",
    "                  \"aapl\":\"apple\",\n",
    "                  \"tsla\":\"tesla\",\n",
    "                  \"msft\":\"microsoft\",\n",
    "                  \"fb\":\"facebook\",\n",
    "                  \"googl\":\"google\",\n",
    "                  \"nio\":\"nio\",\n",
    "                  \"twtr\":\"twitter\",\n",
    "                  \"nflx\":\"netflix\"\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_filter_lemmatize(msg):\n",
    "    words = word_tokenize(msg)\n",
    "    \n",
    "    processed = []\n",
    "    \n",
    "    for w in words:\n",
    "        if (w not in stop_words) and (w not in string.punctuation):\n",
    "            if (w in company_ticker):\n",
    "                w = company_ticker[w]\n",
    "            processed.append(lem.lemmatize(w.lower()))\n",
    "            \n",
    "    return processed\n",
    "\n",
    "tweets['Words'] = tweets['Message'].apply(token_filter_lemmatize)\n",
    "\n",
    "#tweets.to_csv(r'Database.csv', index=False)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[[\"Doc_id\", \"datetime_score\", \"Words\"]]\n",
    "tweets.to_csv(r'tweets_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inverted_Index and Search (START HERE - REUBEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tweets_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_id</th>\n",
       "      <th>datetime_score</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>268864270</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[tsla, well, fargo, 39, top, prediction, 2021,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>268864163</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>[nio, 60, jan, 9th]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>268864120</td>\n",
       "      <td>0.999937</td>\n",
       "      <td>[tsla, opps, cut, head, wierd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>268864106</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>[amzn, sentiment, highly, favorable, right, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>268864074</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>[baba, buying, baba, 350, 400, next, year, mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>268864073</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>[tsla, broke, bear, ran, well, today, year, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>268864071</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>[smartoptions, unusual, activity, alert, delay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>268864047</td>\n",
       "      <td>0.999907</td>\n",
       "      <td>[tsla, happy, new, year, stay, positive, stay,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>268863937</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>[tsla, 750, soon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>268863929</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>[aapl, spy, qqq, will, aapl, hit, 3, trillion,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>268863920</td>\n",
       "      <td>0.999840</td>\n",
       "      <td>[aapl, would, think, perfect, day, buy, call, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>268863912</td>\n",
       "      <td>0.999837</td>\n",
       "      <td>[tsla, delivery, number, 4th, morning, evening]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>268863836</td>\n",
       "      <td>0.999801</td>\n",
       "      <td>[tsla, happy, new, year]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>268863822</td>\n",
       "      <td>0.999795</td>\n",
       "      <td>[amzn, mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>268863735</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>[ostk, e, commerce, today, amzn, etsy, shop, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>268863686</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>[amzn, sequence, analysis, dec, 31, 31, sequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>268863653</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>[aapl, buy, price, apple, 64, wait, see, new, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>268863630</td>\n",
       "      <td>0.999709</td>\n",
       "      <td>[tsla, predict, elon, fly, one, shareholder, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>268863594</td>\n",
       "      <td>0.999697</td>\n",
       "      <td>[aapl, get, hi, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>268863576</td>\n",
       "      <td>0.999690</td>\n",
       "      <td>[emboscado2010, 25, mean, anything, tsla, like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>268863536</td>\n",
       "      <td>0.999674</td>\n",
       "      <td>[the, bubble, may, not, be, what, you, think, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>268863437</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>[supajoe, tbh, dip, 22, 19, hardly, registered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>268863425</td>\n",
       "      <td>0.999628</td>\n",
       "      <td>[spy, w, cheered, ma, amp, treated, like, king...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>268863375</td>\n",
       "      <td>0.999609</td>\n",
       "      <td>[tsla, thank, elon, what, year, congrats, than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>268863320</td>\n",
       "      <td>0.999584</td>\n",
       "      <td>[tsla, nvda, stock, gain, 2020, tesla, 740, et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>268863292</td>\n",
       "      <td>0.999573</td>\n",
       "      <td>[tsla, you, are, high, already, happy, new, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>268863001</td>\n",
       "      <td>0.999449</td>\n",
       "      <td>[tsla, solar, hype, crash, of, 2014, 3d, print...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>268862988</td>\n",
       "      <td>0.999445</td>\n",
       "      <td>[nio, happy, new, year, lady, gent, in, 2021, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>268862975</td>\n",
       "      <td>0.999438</td>\n",
       "      <td>[tsla, monday, will, be, the, day, that, i, sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>268862960</td>\n",
       "      <td>0.999432</td>\n",
       "      <td>[aapl, happy, new, year, by, melbenross, apple]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99970</th>\n",
       "      <td>265941088</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>[tsla, new, strain, covid, 21, bill, gate, quo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99971</th>\n",
       "      <td>265941050</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>[tsla, reflected, fyi, 35, prehour]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99972</th>\n",
       "      <td>265941007</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>[sunw, cbat, trch, these, necessary, company, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99973</th>\n",
       "      <td>265940994</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>[dfen, nio, inc, http, youtu, j5czits7nay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>265940988</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>[tsla, is, current, vaccine, going, work, new,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>265940983</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>[tsla]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>265940952</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>[aapl, temporarily, shut, store, california, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>265940882</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>[tsla, feel, weak, could, collapse, open]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>265940853</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>[tsla, going, bell, sell, rally, vwap]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>265940852</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>[tsla, would, big, money, go, far, creating, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>265940839</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>[amzn, santa, rally]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>265940788</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>[tsla, i, bet, new, strain, uk, also, america,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>265940756</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>[tsla, base, friday, moved, 625, 610, deck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>265940686</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>[tsla, tesla, 39, market, outside, north, amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>265940667</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>[amzn, u, hear, swooshing, sound, like, air, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>265940631</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>[tsla, there, index, fund, money, flowing, any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>265940623</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>[waste, not, want, not, with, new, world, with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>265940614</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>[nio, yuanita, put, printing, doe, swimming, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>265940608</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>[hahaha, glad, i, swing, trade, tsla, weekend,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99989</th>\n",
       "      <td>265940599</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>[tsla, even, drop, 600, week, big, since, sp50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99990</th>\n",
       "      <td>265940594</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>[tsla]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>265940536</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>[tsla, shorted, tsla, fri, eve, looking, good,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>265940510</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>[nio, thee, reason, dropping, currently, bear,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99993</th>\n",
       "      <td>265940492</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>[tsla, elon, rescue]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>265940481</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>[tesla, gene, munster, shameless, pumper, b, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>265940469</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>[baba, one, mentioned, r, wallstreetbets, last...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>265940439</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>[theresafreeman, must, held, tsla]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>265940421</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>[tsla, today, going, like]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>265940404</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>[nio, http, www, healthline, com, health, news...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>265940395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[nio, one, mentioned, r, wallstreetbets, last,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Doc_id  datetime_score  \\\n",
       "0      268864270        1.000000   \n",
       "1      268864163        0.999956   \n",
       "2      268864120        0.999937   \n",
       "3      268864106        0.999932   \n",
       "4      268864074        0.999916   \n",
       "5      268864073        0.999915   \n",
       "6      268864071        0.999915   \n",
       "7      268864047        0.999907   \n",
       "8      268863937        0.999852   \n",
       "9      268863929        0.999850   \n",
       "10     268863920        0.999840   \n",
       "11     268863912        0.999837   \n",
       "12     268863836        0.999801   \n",
       "13     268863822        0.999795   \n",
       "14     268863735        0.999755   \n",
       "15     268863686        0.999734   \n",
       "16     268863653        0.999720   \n",
       "17     268863630        0.999709   \n",
       "18     268863594        0.999697   \n",
       "19     268863576        0.999690   \n",
       "20     268863536        0.999674   \n",
       "21     268863437        0.999634   \n",
       "22     268863425        0.999628   \n",
       "23     268863375        0.999609   \n",
       "24     268863320        0.999584   \n",
       "25     268863292        0.999573   \n",
       "26     268863001        0.999449   \n",
       "27     268862988        0.999445   \n",
       "28     268862975        0.999438   \n",
       "29     268862960        0.999432   \n",
       "...          ...             ...   \n",
       "99970  265941088        0.000169   \n",
       "99971  265941050        0.000160   \n",
       "99972  265941007        0.000145   \n",
       "99973  265940994        0.000142   \n",
       "99974  265940988        0.000142   \n",
       "99975  265940983        0.000140   \n",
       "99976  265940952        0.000133   \n",
       "99977  265940882        0.000120   \n",
       "99978  265940853        0.000113   \n",
       "99979  265940852        0.000113   \n",
       "99980  265940839        0.000110   \n",
       "99981  265940788        0.000098   \n",
       "99982  265940756        0.000088   \n",
       "99983  265940686        0.000068   \n",
       "99984  265940667        0.000063   \n",
       "99985  265940631        0.000054   \n",
       "99986  265940623        0.000052   \n",
       "99987  265940614        0.000051   \n",
       "99988  265940608        0.000050   \n",
       "99989  265940599        0.000048   \n",
       "99990  265940594        0.000047   \n",
       "99991  265940536        0.000031   \n",
       "99992  265940510        0.000027   \n",
       "99993  265940492        0.000021   \n",
       "99994  265940481        0.000017   \n",
       "99995  265940469        0.000014   \n",
       "99996  265940439        0.000008   \n",
       "99997  265940421        0.000004   \n",
       "99998  265940404        0.000001   \n",
       "99999  265940395        0.000000   \n",
       "\n",
       "                                                   Words  \n",
       "0      [tsla, well, fargo, 39, top, prediction, 2021,...  \n",
       "1                                    [nio, 60, jan, 9th]  \n",
       "2                         [tsla, opps, cut, head, wierd]  \n",
       "3      [amzn, sentiment, highly, favorable, right, ne...  \n",
       "4      [baba, buying, baba, 350, 400, next, year, mar...  \n",
       "5      [tsla, broke, bear, ran, well, today, year, ye...  \n",
       "6      [smartoptions, unusual, activity, alert, delay...  \n",
       "7      [tsla, happy, new, year, stay, positive, stay,...  \n",
       "8                                      [tsla, 750, soon]  \n",
       "9      [aapl, spy, qqq, will, aapl, hit, 3, trillion,...  \n",
       "10     [aapl, would, think, perfect, day, buy, call, ...  \n",
       "11       [tsla, delivery, number, 4th, morning, evening]  \n",
       "12                              [tsla, happy, new, year]  \n",
       "13                                          [amzn, mean]  \n",
       "14     [ostk, e, commerce, today, amzn, etsy, shop, w...  \n",
       "15     [amzn, sequence, analysis, dec, 31, 31, sequen...  \n",
       "16     [aapl, buy, price, apple, 64, wait, see, new, ...  \n",
       "17     [tsla, predict, elon, fly, one, shareholder, m...  \n",
       "18                                    [aapl, get, hi, 5]  \n",
       "19     [emboscado2010, 25, mean, anything, tsla, like...  \n",
       "20     [the, bubble, may, not, be, what, you, think, ...  \n",
       "21     [supajoe, tbh, dip, 22, 19, hardly, registered...  \n",
       "22     [spy, w, cheered, ma, amp, treated, like, king...  \n",
       "23     [tsla, thank, elon, what, year, congrats, than...  \n",
       "24     [tsla, nvda, stock, gain, 2020, tesla, 740, et...  \n",
       "25     [tsla, you, are, high, already, happy, new, ye...  \n",
       "26     [tsla, solar, hype, crash, of, 2014, 3d, print...  \n",
       "27     [nio, happy, new, year, lady, gent, in, 2021, ...  \n",
       "28     [tsla, monday, will, be, the, day, that, i, sh...  \n",
       "29       [aapl, happy, new, year, by, melbenross, apple]  \n",
       "...                                                  ...  \n",
       "99970  [tsla, new, strain, covid, 21, bill, gate, quo...  \n",
       "99971                [tsla, reflected, fyi, 35, prehour]  \n",
       "99972  [sunw, cbat, trch, these, necessary, company, ...  \n",
       "99973         [dfen, nio, inc, http, youtu, j5czits7nay]  \n",
       "99974  [tsla, is, current, vaccine, going, work, new,...  \n",
       "99975                                             [tsla]  \n",
       "99976  [aapl, temporarily, shut, store, california, d...  \n",
       "99977          [tsla, feel, weak, could, collapse, open]  \n",
       "99978             [tsla, going, bell, sell, rally, vwap]  \n",
       "99979  [tsla, would, big, money, go, far, creating, f...  \n",
       "99980                               [amzn, santa, rally]  \n",
       "99981  [tsla, i, bet, new, strain, uk, also, america,...  \n",
       "99982        [tsla, base, friday, moved, 625, 610, deck]  \n",
       "99983  [tsla, tesla, 39, market, outside, north, amer...  \n",
       "99984  [amzn, u, hear, swooshing, sound, like, air, g...  \n",
       "99985  [tsla, there, index, fund, money, flowing, any...  \n",
       "99986  [waste, not, want, not, with, new, world, with...  \n",
       "99987  [nio, yuanita, put, printing, doe, swimming, y...  \n",
       "99988  [hahaha, glad, i, swing, trade, tsla, weekend,...  \n",
       "99989  [tsla, even, drop, 600, week, big, since, sp50...  \n",
       "99990                                             [tsla]  \n",
       "99991  [tsla, shorted, tsla, fri, eve, looking, good,...  \n",
       "99992  [nio, thee, reason, dropping, currently, bear,...  \n",
       "99993                               [tsla, elon, rescue]  \n",
       "99994  [tesla, gene, munster, shameless, pumper, b, t...  \n",
       "99995  [baba, one, mentioned, r, wallstreetbets, last...  \n",
       "99996                 [theresafreeman, must, held, tsla]  \n",
       "99997                         [tsla, today, going, like]  \n",
       "99998  [nio, http, www, healthline, com, health, news...  \n",
       "99999  [nio, one, mentioned, r, wallstreetbets, last,...  \n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def read_list(string):\n",
    "    return ast.literal_eval(string)\n",
    "\n",
    "tweets['Words'] = tweets['Words'].apply(read_list)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Inverted Index\n",
    "\n",
    "# FORMAT (Preparing for JSON):\n",
    "\n",
    "# Inv_Index -> Words -> Documents -> Freq + Tf.idf\n",
    "\n",
    "inverted_index = {}\n",
    "\n",
    "docIndex = tweets.columns.get_loc(\"Doc_id\")\n",
    "wordsIndex = tweets.columns.get_loc(\"Words\")\n",
    "\n",
    "for index, doc in tweets.iterrows():\n",
    "    docID = doc[docIndex]\n",
    "    words = doc[wordsIndex]\n",
    "    \n",
    "    for w in words:\n",
    "\n",
    "        if w in company_ticker:\n",
    "            w = company_ticker[w]\n",
    "        \n",
    "        if w not in inverted_index:\n",
    "            inverted_index[w] = {}\n",
    "        \n",
    "        if docID not in inverted_index[w]:\n",
    "            inverted_index[w][docID] = {'freq': 1, 'score':0}\n",
    "        else:\n",
    "            inverted_index[w][docID]['freq'] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>datetime_score</th>\n",
       "      <th>len2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>268864270</th>\n",
       "      <td>[tsla, well, fargo, 39, top, prediction, 2021,...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268864163</th>\n",
       "      <td>[nio, 60, jan, 9th]</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268864120</th>\n",
       "      <td>[tsla, opps, cut, head, wierd]</td>\n",
       "      <td>0.999937</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268864106</th>\n",
       "      <td>[amzn, sentiment, highly, favorable, right, ne...</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268864074</th>\n",
       "      <td>[baba, buying, baba, 350, 400, next, year, mar...</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268864073</th>\n",
       "      <td>[tsla, broke, bear, ran, well, today, year, ye...</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268864071</th>\n",
       "      <td>[smartoptions, unusual, activity, alert, delay...</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268864047</th>\n",
       "      <td>[tsla, happy, new, year, stay, positive, stay,...</td>\n",
       "      <td>0.999907</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863937</th>\n",
       "      <td>[tsla, 750, soon]</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863929</th>\n",
       "      <td>[aapl, spy, qqq, will, aapl, hit, 3, trillion,...</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863920</th>\n",
       "      <td>[aapl, would, think, perfect, day, buy, call, ...</td>\n",
       "      <td>0.999840</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863912</th>\n",
       "      <td>[tsla, delivery, number, 4th, morning, evening]</td>\n",
       "      <td>0.999837</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863836</th>\n",
       "      <td>[tsla, happy, new, year]</td>\n",
       "      <td>0.999801</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863822</th>\n",
       "      <td>[amzn, mean]</td>\n",
       "      <td>0.999795</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863735</th>\n",
       "      <td>[ostk, e, commerce, today, amzn, etsy, shop, w...</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863686</th>\n",
       "      <td>[amzn, sequence, analysis, dec, 31, 31, sequen...</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863653</th>\n",
       "      <td>[aapl, buy, price, apple, 64, wait, see, new, ...</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863630</th>\n",
       "      <td>[tsla, predict, elon, fly, one, shareholder, m...</td>\n",
       "      <td>0.999709</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863594</th>\n",
       "      <td>[aapl, get, hi, 5]</td>\n",
       "      <td>0.999697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863576</th>\n",
       "      <td>[emboscado2010, 25, mean, anything, tsla, like...</td>\n",
       "      <td>0.999690</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863536</th>\n",
       "      <td>[the, bubble, may, not, be, what, you, think, ...</td>\n",
       "      <td>0.999674</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863437</th>\n",
       "      <td>[supajoe, tbh, dip, 22, 19, hardly, registered...</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863425</th>\n",
       "      <td>[spy, w, cheered, ma, amp, treated, like, king...</td>\n",
       "      <td>0.999628</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863375</th>\n",
       "      <td>[tsla, thank, elon, what, year, congrats, than...</td>\n",
       "      <td>0.999609</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863320</th>\n",
       "      <td>[tsla, nvda, stock, gain, 2020, tesla, 740, et...</td>\n",
       "      <td>0.999584</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863292</th>\n",
       "      <td>[tsla, you, are, high, already, happy, new, ye...</td>\n",
       "      <td>0.999573</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268863001</th>\n",
       "      <td>[tsla, solar, hype, crash, of, 2014, 3d, print...</td>\n",
       "      <td>0.999449</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268862988</th>\n",
       "      <td>[nio, happy, new, year, lady, gent, in, 2021, ...</td>\n",
       "      <td>0.999445</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268862975</th>\n",
       "      <td>[tsla, monday, will, be, the, day, that, i, sh...</td>\n",
       "      <td>0.999438</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268862960</th>\n",
       "      <td>[aapl, happy, new, year, by, melbenross, apple]</td>\n",
       "      <td>0.999432</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265941088</th>\n",
       "      <td>[tsla, new, strain, covid, 21, bill, gate, quo...</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265941050</th>\n",
       "      <td>[tsla, reflected, fyi, 35, prehour]</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265941007</th>\n",
       "      <td>[sunw, cbat, trch, these, necessary, company, ...</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940994</th>\n",
       "      <td>[dfen, nio, inc, http, youtu, j5czits7nay]</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940988</th>\n",
       "      <td>[tsla, is, current, vaccine, going, work, new,...</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940983</th>\n",
       "      <td>[tsla]</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940952</th>\n",
       "      <td>[aapl, temporarily, shut, store, california, d...</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940882</th>\n",
       "      <td>[tsla, feel, weak, could, collapse, open]</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940853</th>\n",
       "      <td>[tsla, going, bell, sell, rally, vwap]</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940852</th>\n",
       "      <td>[tsla, would, big, money, go, far, creating, f...</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940839</th>\n",
       "      <td>[amzn, santa, rally]</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940788</th>\n",
       "      <td>[tsla, i, bet, new, strain, uk, also, america,...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940756</th>\n",
       "      <td>[tsla, base, friday, moved, 625, 610, deck]</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940686</th>\n",
       "      <td>[tsla, tesla, 39, market, outside, north, amer...</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940667</th>\n",
       "      <td>[amzn, u, hear, swooshing, sound, like, air, g...</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940631</th>\n",
       "      <td>[tsla, there, index, fund, money, flowing, any...</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940623</th>\n",
       "      <td>[waste, not, want, not, with, new, world, with...</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940614</th>\n",
       "      <td>[nio, yuanita, put, printing, doe, swimming, y...</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940608</th>\n",
       "      <td>[hahaha, glad, i, swing, trade, tsla, weekend,...</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940599</th>\n",
       "      <td>[tsla, even, drop, 600, week, big, since, sp50...</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940594</th>\n",
       "      <td>[tsla]</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940536</th>\n",
       "      <td>[tsla, shorted, tsla, fri, eve, looking, good,...</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940510</th>\n",
       "      <td>[nio, thee, reason, dropping, currently, bear,...</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940492</th>\n",
       "      <td>[tsla, elon, rescue]</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940481</th>\n",
       "      <td>[tesla, gene, munster, shameless, pumper, b, t...</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940469</th>\n",
       "      <td>[baba, one, mentioned, r, wallstreetbets, last...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940439</th>\n",
       "      <td>[theresafreeman, must, held, tsla]</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940421</th>\n",
       "      <td>[tsla, today, going, like]</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940404</th>\n",
       "      <td>[nio, http, www, healthline, com, health, news...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265940395</th>\n",
       "      <td>[nio, one, mentioned, r, wallstreetbets, last,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Words  datetime_score  \\\n",
       "Doc_id                                                                         \n",
       "268864270  [tsla, well, fargo, 39, top, prediction, 2021,...        1.000000   \n",
       "268864163                                [nio, 60, jan, 9th]        0.999956   \n",
       "268864120                     [tsla, opps, cut, head, wierd]        0.999937   \n",
       "268864106  [amzn, sentiment, highly, favorable, right, ne...        0.999932   \n",
       "268864074  [baba, buying, baba, 350, 400, next, year, mar...        0.999916   \n",
       "268864073  [tsla, broke, bear, ran, well, today, year, ye...        0.999915   \n",
       "268864071  [smartoptions, unusual, activity, alert, delay...        0.999915   \n",
       "268864047  [tsla, happy, new, year, stay, positive, stay,...        0.999907   \n",
       "268863937                                  [tsla, 750, soon]        0.999852   \n",
       "268863929  [aapl, spy, qqq, will, aapl, hit, 3, trillion,...        0.999850   \n",
       "268863920  [aapl, would, think, perfect, day, buy, call, ...        0.999840   \n",
       "268863912    [tsla, delivery, number, 4th, morning, evening]        0.999837   \n",
       "268863836                           [tsla, happy, new, year]        0.999801   \n",
       "268863822                                       [amzn, mean]        0.999795   \n",
       "268863735  [ostk, e, commerce, today, amzn, etsy, shop, w...        0.999755   \n",
       "268863686  [amzn, sequence, analysis, dec, 31, 31, sequen...        0.999734   \n",
       "268863653  [aapl, buy, price, apple, 64, wait, see, new, ...        0.999720   \n",
       "268863630  [tsla, predict, elon, fly, one, shareholder, m...        0.999709   \n",
       "268863594                                 [aapl, get, hi, 5]        0.999697   \n",
       "268863576  [emboscado2010, 25, mean, anything, tsla, like...        0.999690   \n",
       "268863536  [the, bubble, may, not, be, what, you, think, ...        0.999674   \n",
       "268863437  [supajoe, tbh, dip, 22, 19, hardly, registered...        0.999634   \n",
       "268863425  [spy, w, cheered, ma, amp, treated, like, king...        0.999628   \n",
       "268863375  [tsla, thank, elon, what, year, congrats, than...        0.999609   \n",
       "268863320  [tsla, nvda, stock, gain, 2020, tesla, 740, et...        0.999584   \n",
       "268863292  [tsla, you, are, high, already, happy, new, ye...        0.999573   \n",
       "268863001  [tsla, solar, hype, crash, of, 2014, 3d, print...        0.999449   \n",
       "268862988  [nio, happy, new, year, lady, gent, in, 2021, ...        0.999445   \n",
       "268862975  [tsla, monday, will, be, the, day, that, i, sh...        0.999438   \n",
       "268862960    [aapl, happy, new, year, by, melbenross, apple]        0.999432   \n",
       "...                                                      ...             ...   \n",
       "265941088  [tsla, new, strain, covid, 21, bill, gate, quo...        0.000169   \n",
       "265941050                [tsla, reflected, fyi, 35, prehour]        0.000160   \n",
       "265941007  [sunw, cbat, trch, these, necessary, company, ...        0.000145   \n",
       "265940994         [dfen, nio, inc, http, youtu, j5czits7nay]        0.000142   \n",
       "265940988  [tsla, is, current, vaccine, going, work, new,...        0.000142   \n",
       "265940983                                             [tsla]        0.000140   \n",
       "265940952  [aapl, temporarily, shut, store, california, d...        0.000133   \n",
       "265940882          [tsla, feel, weak, could, collapse, open]        0.000120   \n",
       "265940853             [tsla, going, bell, sell, rally, vwap]        0.000113   \n",
       "265940852  [tsla, would, big, money, go, far, creating, f...        0.000113   \n",
       "265940839                               [amzn, santa, rally]        0.000110   \n",
       "265940788  [tsla, i, bet, new, strain, uk, also, america,...        0.000098   \n",
       "265940756        [tsla, base, friday, moved, 625, 610, deck]        0.000088   \n",
       "265940686  [tsla, tesla, 39, market, outside, north, amer...        0.000068   \n",
       "265940667  [amzn, u, hear, swooshing, sound, like, air, g...        0.000063   \n",
       "265940631  [tsla, there, index, fund, money, flowing, any...        0.000054   \n",
       "265940623  [waste, not, want, not, with, new, world, with...        0.000052   \n",
       "265940614  [nio, yuanita, put, printing, doe, swimming, y...        0.000051   \n",
       "265940608  [hahaha, glad, i, swing, trade, tsla, weekend,...        0.000050   \n",
       "265940599  [tsla, even, drop, 600, week, big, since, sp50...        0.000048   \n",
       "265940594                                             [tsla]        0.000047   \n",
       "265940536  [tsla, shorted, tsla, fri, eve, looking, good,...        0.000031   \n",
       "265940510  [nio, thee, reason, dropping, currently, bear,...        0.000027   \n",
       "265940492                               [tsla, elon, rescue]        0.000021   \n",
       "265940481  [tesla, gene, munster, shameless, pumper, b, t...        0.000017   \n",
       "265940469  [baba, one, mentioned, r, wallstreetbets, last...        0.000014   \n",
       "265940439                 [theresafreeman, must, held, tsla]        0.000008   \n",
       "265940421                         [tsla, today, going, like]        0.000004   \n",
       "265940404  [nio, http, www, healthline, com, health, news...        0.000001   \n",
       "265940395  [nio, one, mentioned, r, wallstreetbets, last,...        0.000000   \n",
       "\n",
       "           len2  \n",
       "Doc_id           \n",
       "268864270   0.0  \n",
       "268864163   0.0  \n",
       "268864120   0.0  \n",
       "268864106   0.0  \n",
       "268864074   0.0  \n",
       "268864073   0.0  \n",
       "268864071   0.0  \n",
       "268864047   0.0  \n",
       "268863937   0.0  \n",
       "268863929   0.0  \n",
       "268863920   0.0  \n",
       "268863912   0.0  \n",
       "268863836   0.0  \n",
       "268863822   0.0  \n",
       "268863735   0.0  \n",
       "268863686   0.0  \n",
       "268863653   0.0  \n",
       "268863630   0.0  \n",
       "268863594   0.0  \n",
       "268863576   0.0  \n",
       "268863536   0.0  \n",
       "268863437   0.0  \n",
       "268863425   0.0  \n",
       "268863375   0.0  \n",
       "268863320   0.0  \n",
       "268863292   0.0  \n",
       "268863001   0.0  \n",
       "268862988   0.0  \n",
       "268862975   0.0  \n",
       "268862960   0.0  \n",
       "...         ...  \n",
       "265941088   0.0  \n",
       "265941050   0.0  \n",
       "265941007   0.0  \n",
       "265940994   0.0  \n",
       "265940988   0.0  \n",
       "265940983   0.0  \n",
       "265940952   0.0  \n",
       "265940882   0.0  \n",
       "265940853   0.0  \n",
       "265940852   0.0  \n",
       "265940839   0.0  \n",
       "265940788   0.0  \n",
       "265940756   0.0  \n",
       "265940686   0.0  \n",
       "265940667   0.0  \n",
       "265940631   0.0  \n",
       "265940623   0.0  \n",
       "265940614   0.0  \n",
       "265940608   0.0  \n",
       "265940599   0.0  \n",
       "265940594   0.0  \n",
       "265940536   0.0  \n",
       "265940510   0.0  \n",
       "265940492   0.0  \n",
       "265940481   0.0  \n",
       "265940469   0.0  \n",
       "265940439   0.0  \n",
       "265940421   0.0  \n",
       "265940404   0.0  \n",
       "265940395   0.0  \n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_docid = tweets.set_index(\"Doc_id\")[['Words','datetime_score']]\n",
    "tweets_docid['len2'] = 0.0\n",
    "tweets_docid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38650"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chavalitpramotedham/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# CALCULATE TF.IDF\n",
    "## TF = (count of word in doc)/(total words in doc)\n",
    "## IDF = 1\n",
    "\n",
    "import math\n",
    "\n",
    "def tf(word_dict, docID):\n",
    "    count = word_dict[docID]['freq']\n",
    "    return (1 + math.log(count,10))\n",
    "    \n",
    "def get_datetime_score(docID):\n",
    "    return tweets_docid['datetime_score'][docID]\n",
    "\n",
    "num = 0\n",
    "\n",
    "for word in inverted_index:\n",
    "    # Using lnc.ltc\n",
    "    ## n means idf_value = 1\n",
    "    \n",
    "    idf_value = 1\n",
    "    \n",
    "    for docID in inverted_index[word]:\n",
    "        tf_value = tf(inverted_index[word], docID)\n",
    "        del inverted_index[word][docID]['freq']\n",
    "        \n",
    "        tweets_docid['len2'][docID] += (tf_value)**2\n",
    "        \n",
    "        tf_idf = tf_value * idf_value\n",
    "        \n",
    "        inverted_index[word][docID]['tf.idf'] = tf_idf\n",
    "    \n",
    "    num += 1\n",
    "    \n",
    "    if (num % 20 == 0):\n",
    "        print(\"Completed: \"+str(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_norm(length):\n",
    "    return 1/((length)**(1/2))\n",
    "\n",
    "tweets_docid['norm_coeff'] = tweets_docid['len2'].apply(calc_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_docid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_scores = {}\n",
    "\n",
    "dtIndex = tweets_docid.columns.get_loc(\"datetime_score\")\n",
    "normIndex = tweets_docid.columns.get_loc(\"norm_coeff\")\n",
    "\n",
    "for index, doc in tweets_docid.iterrows():\n",
    "    dtScore = doc[dtIndex]\n",
    "    norm_coeff = doc[normIndex]\n",
    "    \n",
    "    tweets_scores[index] = {}\n",
    "    tweets_scores[index]['datetime_score'] = dtScore\n",
    "    tweets_scores[index]['norm_coeff'] = norm_coeff\n",
    "    \n",
    "    \n",
    "def store_tweets_scores():\n",
    "    with open('tweets_scores.json','w') as outfile:\n",
    "        json.dump(tweets_scores,outfile)\n",
    "    \n",
    "store_tweets_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tiered index:\n",
    "\n",
    "champion_inverted_index = {}\n",
    "\n",
    "num = 0\n",
    "\n",
    "for word in inverted_index:\n",
    "    \n",
    "    champion_len = 500\n",
    "    \n",
    "    for word in inverted_index:\n",
    "        \n",
    "        champion_inverted_index[word] = {}\n",
    "        \n",
    "        if (len(inverted_index[word]) > champion_len):\n",
    "            sort_word = sorted(inverted_index[word].items(), key=lambda item: item[1]['tf.idf'], reverse = True)\n",
    "            \n",
    "            for i in sort_word[:champion_len]:\n",
    "                champion_inverted_index[word][i[0]] = i[1]['tf.idf']\n",
    "            \n",
    "        else:\n",
    "            for i in inverted_index[word].items():\n",
    "                champion_inverted_index[word][i[0]] = i[1]['tf.idf']\n",
    "                \n",
    "        champion_inverted_index[word]['count'] = len(inverted_index[word])\n",
    "    \n",
    "    num += 1\n",
    "    \n",
    "    if (num % 20 == 0):\n",
    "        print(\"Completed: \"+str(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def store_inv_index():\n",
    "    with open('InvIndex.json', 'w') as outfile:\n",
    "        json.dump(inverted_index,outfile)\n",
    "        \n",
    "def store_champion_list():\n",
    "    with open('ChampionList.json','w') as outfile:\n",
    "        json.dump(champion_inverted_index,outfile)\n",
    "\n",
    "# store_inv_index()\n",
    "store_champion_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInvIndex():\n",
    "    with open('tiered_InvIndex.json') as infile:\n",
    "        data = json.load(infile)\n",
    "    return data\n",
    "\n",
    "inv_index = getInvIndex()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask_restful import Api, Resource\n",
    "from flask import request\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "import pyodbc\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "api = Api(app)\n",
    "\n",
    "# conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=DESKTOP-97U22LI;DATABASE=CZ4034;UID=sa;PWD=password1')\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "class SearchResource(Resource):    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.ChampionList = defaultdict(set, self.getChampionList())\n",
    "#         self.InvIndex = defaultdict(set, self.getInvIndex())\n",
    "        \n",
    "        self.words = pd.Series(list(self.ChampionList.keys()))\n",
    "        self.company_ticker = {\"baba\":\"alibaba\",\n",
    "                              \"amzn\":\"amazon\",\n",
    "                              \"aapl\":\"apple\",\n",
    "                              \"tsla\":\"tesla\",\n",
    "                              \"msft\":\"microsoft\",\n",
    "                              \"fb\":\"facebook\",\n",
    "                              \"googl\":\"google\",\n",
    "                              \"nio\":\"nio\",\n",
    "                              \"twtr\":\"twitter\",\n",
    "                              \"nflx\":\"netflix\"\n",
    "                             }\n",
    "        \n",
    "        self.tweets_scores = self.getTweetsScores()\n",
    "        \n",
    "        # CHANGE PARAM WHEN MORE SCRAPED!\n",
    "        self.num_docs = len(self.tweets_scores)\n",
    "        \n",
    "    ################# API FUNCTIONS #######################\n",
    "        \n",
    "    def get(self):\n",
    "        query = request.args.get('query')\n",
    "        matched_index = self.search(query)\n",
    "        \n",
    "        data = self.queryDB('tweets',matched_index).to_json() \n",
    "        \n",
    "        return {'content': data}\n",
    "    \n",
    "    def post(self):\n",
    "        return {\"data\": \"posted!\"}\n",
    "    \n",
    "    ################# QUERY SEARCH #######################\n",
    "    \n",
    "    def clean_query(self, query, exact):\n",
    "        query_clean = []\n",
    "        \n",
    "        query_split = word_tokenize(query)\n",
    "        \n",
    "        for query_word in query_split:\n",
    "            word_lower = query_word.lower()\n",
    "            if word_lower not in stop_words:\n",
    "                cur = lem.lemmatize(word_lower)\n",
    "                \n",
    "                if cur in self.company_ticker:\n",
    "                    cur = self.company_ticker[cur]\n",
    "                    \n",
    "                if not exact:\n",
    "                    cur = self.JDreco(cur)\n",
    "                \n",
    "                if (cur not in query_clean):\n",
    "                    query_clean.append(cur)\n",
    "                    \n",
    "        return query_clean\n",
    "    \n",
    "    def search(self, query, exact = False):\n",
    "        \n",
    "        # Returns 200 (or less if unavailable) most relevant documents\n",
    "        \n",
    "        matched_index_list = []\n",
    "        matched_documents = None\n",
    "        \n",
    "        # (1) Clean Query:\n",
    "        query_clean = self.clean_query(query, exact)\n",
    "        print(\"Recommended Query List:\")\n",
    "        print(query_clean)\n",
    "        \n",
    "        \n",
    "        # (2) Find All Docs (match any one word in query):\n",
    "        for word in query_clean:\n",
    "            if word in self.ChampionList:\n",
    "                matches = self.ChampionList[word]\n",
    "                \n",
    "                match_index = list(matches.keys())\n",
    "                match_index.remove('count')\n",
    "            \n",
    "            else:\n",
    "                print(\"No Matches for Word: \"+word)\n",
    "                continue\n",
    "    \n",
    "#             matched_index_list.append(match_index)\n",
    "            \n",
    "            if matched_documents is None:\n",
    "                matched_documents = match_index\n",
    "                \n",
    "            else:\n",
    "                #matched_documents = list(set.intersection(set(matched_documents),set(match_index)))\n",
    "                matched_documents = list(set.union(set(matched_documents),set(match_index)))\n",
    "                \n",
    "                \n",
    "        # (3) Filter Most Relevant:\n",
    "        \n",
    "        filter_amt = 200\n",
    "\n",
    "        weights = self.tf_idf(query_clean)\n",
    "            \n",
    "        doc_score = {}\n",
    "            \n",
    "        for docID in matched_documents:\n",
    "            doc_score[docID] = self.calc_match_score(weights, docID)\n",
    "                \n",
    "        matched_documents = sorted(doc_score, key=doc_score.get, reverse = True)\n",
    "            \n",
    "        if (filter_amt <= len(matched_documents)):\n",
    "            matched_documents = matched_documents[:filter_amt]\n",
    "        \n",
    "        # (4) Return Results:\n",
    "        return matched_documents\n",
    "    \n",
    "    ################# RECOMMENDATION #######################\n",
    "\n",
    "    def jaccard(self,entry, gram_number):\n",
    "        spellings = self.words[self.words.str.startswith(entry[0])]\n",
    "        distances = ((jaccard_distance(set(ngrams(entry,gram_number)),\n",
    "                                           set(ngrams(word,gram_number))), word)\n",
    "                     for word in spellings)\n",
    "        closest = min(distances)\n",
    "        return closest[1]\n",
    "    \n",
    "    \n",
    "    def JDreco(self,entry):\n",
    "        return self.jaccard(entry, 2)\n",
    "    \n",
    "    ################# TF.IDF #######################\n",
    "    \n",
    "    def tf_idf(self,query):\n",
    "        weights = {}\n",
    "        \n",
    "        len2 = 0\n",
    "        \n",
    "        for word in query:\n",
    "            \n",
    "            # Find IDF value\n",
    "            if (word in self.ChampionList):\n",
    "                idf_value = self.idf(self.ChampionList[word])\n",
    "            else:\n",
    "                idf_value = 0\n",
    "                \n",
    "            # Find TF Value\n",
    "            tf_value = 1 + math.log(1,10) # = 1\n",
    "            \n",
    "            # Store TF.IDF\n",
    "            tf_idf = tf_value * idf_value\n",
    "            \n",
    "            weights[word] = tf_idf\n",
    "            len2 += tf_idf**2\n",
    "            \n",
    "        norm = 1/(len2**(1/2))\n",
    "        \n",
    "        for word in weights:\n",
    "            weights[word] = weights[word] * norm\n",
    "        \n",
    "        return weights\n",
    "            \n",
    "    \n",
    "    def idf(self,word_dict):\n",
    "        value = self.num_docs/word_dict['count']\n",
    "        return math.log(value, 10)\n",
    "    \n",
    "    ################# FILTER MOST RELEVANT #######################\n",
    "    \n",
    "    def calc_match_score(self, weights_query, docID):\n",
    "        \n",
    "        result = 0\n",
    "        \n",
    "#         print(self.tweets_scores)\n",
    "        \n",
    "        norm = float(self.tweets_scores[str(docID)]['norm_coeff'])\n",
    "        dtScore = float(self.tweets_scores[str(docID)]['datetime_score'])\n",
    "        recency_weight = 0.33\n",
    "        \n",
    "        for word in weights_query:\n",
    "\n",
    "            #QUERY\n",
    "            query_score = weights_query[word]\n",
    "            \n",
    "            #DOC\n",
    "            try:\n",
    "                doc_score = float(self.ChampionList[word][docID]) * norm\n",
    "            except:\n",
    "                doc_score = 0\n",
    "            \n",
    "            #PRODUCT\n",
    "            result += query_score * doc_score\n",
    "            \n",
    "        result += dtScore * recency_weight\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    ################# INVERTED INDEX #######################\n",
    "    \n",
    "    def getInvIndex(self):\n",
    "        with open('InvIndex.json') as infile:\n",
    "            data = json.load(infile)\n",
    "        return data\n",
    "    \n",
    "    def getChampionList(self):\n",
    "        with open('ChampionList.json') as infile:\n",
    "            data = json.load(infile)\n",
    "        return data\n",
    "    \n",
    "    def getTweetsScores(self):\n",
    "        with open('tweets_scores.json') as infile:\n",
    "            data = json.load(infile)\n",
    "        return data\n",
    "    \n",
    "    ################# DATABASE CONNECTION & QUERY #######################\n",
    "\n",
    "    def queryDB(self,table, dbQuery):\n",
    "        return pd.read_sql_query('SELECT * FROM CZ4034.dbo.'+table+' where Doc_id IN '+str(tuple(dbQuery)),conn)\n",
    "    \n",
    "    def queryOffline(self, query, exact = False):\n",
    "        match_list = self.search(query, exact = exact)\n",
    "        \n",
    "        for docID in match_list:\n",
    "            print(tweets[tweets['Doc_id']==int(docID)]['Message'])\n",
    "\n",
    "api.add_resource(SearchResource, \"/search\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = SearchResource()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.search(\"stonks best apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "inverted_index = defaultdict(set)\n",
    "# bigram_inverted_index = defaultdict(set)\n",
    "\n",
    "docIndex = tweets.columns.get_loc(\"Doc_id\")\n",
    "wordsIndex = tweets.columns.get_loc(\"Words\")\n",
    "\n",
    "for index, doc in tweets.iterrows():\n",
    "    docID = doc[docIndex]\n",
    "    words = doc[wordsIndex]\n",
    "    \n",
    "    i = 0\n",
    "#     first = True\n",
    "    \n",
    "    for w in words:\n",
    "        \n",
    "        if w in company_ticker:\n",
    "            w = company_ticker[w]\n",
    "        \n",
    "        inverted_index[w].add((docID,i))\n",
    "        \n",
    "#         if not first:\n",
    "#             bigram = prev+' '+cur\n",
    "#             bigram_inverted_index[bigram].add((docID,i-1))\n",
    "            \n",
    "#         first = False\n",
    "#         prev = cur\n",
    "\n",
    "        i+=1\n",
    "    \n",
    "inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_words = pd.Series(list(inverted_index.keys()))\n",
    "\n",
    "def search(query, df = tweets):\n",
    "    matched_documents = set()\n",
    "    \n",
    "    words = word_tokenize(query)\n",
    "    \n",
    "    prev = None\n",
    "    \n",
    "    for word in words:\n",
    "        word_lower = word.lower()\n",
    "        if word_lower not in stop_words:\n",
    "            cur = lem.lemmatize(word_lower)\n",
    "            \n",
    "            reco_cur = JDreco(cur)\n",
    "            print(reco_cur)\n",
    "            \n",
    "            match1 = inverted_index.get(reco_cur)\n",
    "\n",
    "            if match1:\n",
    "                # The operator |= is a short hand for set union\n",
    "                matched_documents |= match1\n",
    "                \n",
    "    match_index = [item[0] for item in matched_documents]\n",
    "    return df[df['Doc_id'].isin(match_index)]\n",
    "\n",
    "\n",
    "def jaccard(entry, gram_number):\n",
    "    spellings = available_words[available_words.str.startswith(entry[0])]\n",
    "    distances = ((jaccard_distance(set(ngrams(entry,gram_number)),\n",
    "                                       set(ngrams(word,gram_number))), word)\n",
    "                 for word in spellings)\n",
    "\n",
    "    closest = min(distances)\n",
    "    return closest[1]\n",
    "\n",
    "\n",
    "def JDreco(entry):\n",
    "    return jaccard(entry, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, df = tweets, exact = False):\n",
    "    matched_documents = set()\n",
    "    \n",
    "    words = word_tokenize(query)\n",
    "    \n",
    "    prev = None\n",
    "    \n",
    "    for word in words:\n",
    "        word_lower = word.lower()\n",
    "        if word_lower not in stop_words:\n",
    "            cur = lem.lemmatize(word_lower)\n",
    "            \n",
    "            if cur in company_ticker:\n",
    "                cur = company_ticker[cur]\n",
    "            \n",
    "            reco_cur = JDreco(cur)\n",
    "            \n",
    "            matches = inverted_index.get(reco_cur)\n",
    "\n",
    "            if matches:\n",
    "                # The operator |= is a short hand for set union\n",
    "                matched_documents |= matches\n",
    "                \n",
    "    \n",
    "                \n",
    "    match_index = [item[0] for item in matched_documents]\n",
    "    return match_index #df[df['Doc_id'].isin(match_index)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
